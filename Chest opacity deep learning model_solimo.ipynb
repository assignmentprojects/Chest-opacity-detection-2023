{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input,Dropout,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory for working with the data\n",
    "\n",
    "if not os.path.exists('E:/python/deep_learning_project/dataset1/datasets'):\n",
    "    os.makedirs('E:/python/deep_learning_project/dataset1/datasets') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory for working with the data\n",
    "\n",
    "if not os.path.exists('E:/python/deep_learning_project/dataset1/datasets/training'):\n",
    "    os.makedirs('E:/python/deep_learning_project/dataset1/datasets/training') \n",
    "\n",
    "if not os.path.exists('E:/python/deep_learning_project/dataset1/datasets/testing'):\n",
    "    os.makedirs('E:/python/deep_learning_project/dataset1/datasets/testing') \n",
    "\n",
    "if not os.path.exists('E:/python/deep_learning_project/dataset1/datasets/validation'):\n",
    "    os.makedirs('E:/python/deep_learning_project/dataset1/datasets/validation') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"E:/python/deep_learning_project/dataset1/datasets/training/normal\"):\n",
    "    os.makedirs(\"E:/python/deep_learning_project/dataset1/datasets/training/normal\")\n",
    "if not os.path.exists(\"E:/python/deep_learning_project/dataset1/datasets/training/sick\"):\n",
    "    os.makedirs(\"E:/python/deep_learning_project/dataset1/datasets/training/sick\")\n",
    "if not os.path.exists(\"E:/python/deep_learning_project/dataset1/datasets/testing/normal\"):\n",
    "    os.makedirs(\"E:/python/deep_learning_project/dataset1/datasets/testing/normal\")\n",
    "if not os.path.exists(\"E:/python/deep_learning_project/dataset1/datasets/testing/sick\"):\n",
    "    os.makedirs(\"E:/python/deep_learning_project/dataset1/datasets/testing/sick\")\n",
    "if not os.path.exists(\"E:/python/deep_learning_project/dataset1/datasets/validation/normal\"):\n",
    "    os.makedirs(\"E:/python/deep_learning_project/dataset1/datasets/validation/normal\")\n",
    "if not os.path.exists(\"E:/python/deep_learning_project/dataset1/datasets/validation/sick\"):\n",
    "    os.makedirs(\"E:/python/deep_learning_project/dataset1/datasets/validation/sick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path  = 'E:/python/deep_learning_project/dataset1/known_images'\n",
    "images = os.listdir(main_path)\n",
    "\n",
    "normal_images = []\n",
    "sick_images = []\n",
    "\n",
    "# seperate normal images from sick images\n",
    "for image in images:\n",
    "    if image.startswith('normal'):\n",
    "        normal_images.append(image)\n",
    "    else:\n",
    "        sick_images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test and validation split for normal images\n",
    "def populate_normal_images(normal_images, path):\n",
    "    for i,img in enumerate(normal_images):\n",
    "        if i < 250:\n",
    "            shutil.copy(os.path.join(path, img),\n",
    "                      os.path.join(\"E:/python/deep_learning_project/dataset1/datasets/training/normal\", img))\n",
    "        elif i > 250 and i < (250+50):\n",
    "            shutil.copy(os.path.join(path, img),\n",
    "                      os.path.join(\"E:/python/deep_learning_project/dataset1/datasets/testing/normal\", img))\n",
    "        else:\n",
    "              if i > 364:\n",
    "                return\n",
    "              shutil.copy(os.path.join(path, img),\n",
    "                          os.path.join(\"E:/python/deep_learning_project/dataset1/datasets/validation/normal\", img))\n",
    "    return\n",
    "    \n",
    "populate_normal_images(normal_images,main_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test and validation split for  sick images\n",
    "def populate_sick_images(sick_images,path):\n",
    "    for i,img in enumerate(sick_images):\n",
    "        if i < 250:\n",
    "            shutil.copy(os.path.join(path, img),\n",
    "                      os.path.join(\"E:/python/deep_learning_project/dataset1/datasets/training/sick\", img))\n",
    "        elif i > 250 and i < (250+50):\n",
    "            shutil.copy(os.path.join(path, img),\n",
    "                      os.path.join(\"E:/python/deep_learning_project/dataset1/datasets/testing/sick\", img))\n",
    "        else:\n",
    "              if i > (351):\n",
    "                return\n",
    "              shutil.copy(os.path.join(path, img),\n",
    "                          os.path.join(\"E:/python/deep_learning_project/dataset1/datasets/validation/sick\", img))\n",
    "    return\n",
    "\n",
    "populate_sick_images(sick_images,main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "#IMAGE_SIZE = 224\n",
    "#BATCH_SIZE = 32\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aaa28c",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "img_size = (224, 224)\n",
    "#batch_size = 16\n",
    "#EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ddf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation parameters for the training set\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af155cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "train_set = train_datagen.flow_from_directory('E:/python/deep_learning_project/dataset1/datasets/training',\n",
    "                                              target_size=(224, 224),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bcf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test set\n",
    "test_set = ImageDataGenerator(rescale=1./255).flow_from_directory('E:/python/deep_learning_project/dataset1/datasets/testing',\n",
    "                                                                   #target_size=img_size,\n",
    "                                                                   target_size=(224, 224),\n",
    "                                                                   batch_size=32,\n",
    "                                                                   class_mode='binary',\n",
    "                                                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation set\n",
    "val_set = ImageDataGenerator(rescale=1./255).flow_from_directory('E:/python/deep_learning_project/dataset1/datasets/validation',\n",
    "                                                                  target_size=(224, 224),\n",
    "                                                                  batch_size=32,\n",
    "                                                                  class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7186545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "model = keras.models.Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "Dropout(0.5),\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and setting callbacks for our model\n",
    "early_stopping_monitor = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    ")\n",
    "\n",
    "best_model = ModelCheckpoint('bestmodel.hdf5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "history = model.fit(train_set,\n",
    "           epochs= 10,\n",
    "           batch_size=32,\n",
    "           validation_data=val_set,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d0e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "prediction = model.predict(test_set)\n",
    "y_pred = (prediction > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53251bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, accuracy = model.evaluate(test_set)\n",
    "test_loss, precision = model.evaluate(test_set)\n",
    "test_loss, recall = model.evaluate(test_set)\n",
    "test_loss, f1score = model.evaluate(test_set)\n",
    "print('Accuracy        :',(accuracy))\n",
    "print('Precision       :',(precision))\n",
    "print('Recall          :',(recall))\n",
    "print('F1-score        :',(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39406c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report(pred, test_set):\n",
    "    # Generate status classification report\n",
    "    print(classification_report(pred, test_set.classes))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm_status = confusion_matrix(test_set.classes, pred)\n",
    "\n",
    "    # Plot status confusion matrix as heatmap\n",
    "    sns.heatmap(cm_status, annot=True)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show\n",
    "\n",
    "generate_classification_report(y_pred, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19992a55",
   "metadata": {},
   "source": [
    "## Predicting the unknown images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to data and load into a list\n",
    "unknown_images_path = 'E:/python/deep_learning_project/dataset1/unknown_images'\n",
    "images = os.listdir(unknown_images_path)\n",
    "unknown_images = []\n",
    "for img_sets in images:\n",
    "    unknown_image_path = f'{unknown_images_path}/{img_sets}'\n",
    "    unknown_image = tf.keras.preprocessing.image.load_img(unknown_image_path, target_size=(224, 224))\n",
    "    unknown_image = tf.keras.preprocessing.image.img_to_array(unknown_image)\n",
    "    unknown_image = np.expand_dims(unknown_image, axis=0)\n",
    "    unknown_images.append(unknown_image)\n",
    "\n",
    "# Concatenate the unknown images into an array\n",
    "unknown_images = np.concatenate(unknown_images, axis=0)\n",
    "\n",
    "# Make predictions on the unknown images\n",
    "predictions = model.predict(unknown_images)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "results = []\n",
    "result_labels = []\n",
    "\n",
    "# Print the predictions\n",
    "for i, prediction in enumerate(predictions):\n",
    "    result_labels.append(f'Image {i+1}')\n",
    "    results.append(prediction[0])\n",
    "    \n",
    "classes = np.argmax(predictions, axis = 1)\n",
    "print(classes)\n",
    "\n",
    "#create results dataframe\n",
    "results_df = pd.DataFrame({'Image': result_labels, 'Prediction': results})\n",
    "\n",
    "# save the dataframe to a CSV file\n",
    "#results_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b77981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert array into dataframe\n",
    "DF = pd.DataFrame(predictions)\n",
    "  \n",
    "# save the dataframe as a csv file\n",
    "DF.to_csv(\"image_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, accuracy = model.evaluate(test_set)\n",
    "test_loss, precision = model.evaluate(test_set)\n",
    "test_loss, recall = model.evaluate(test_set)\n",
    "test_loss, f1score = model.evaluate(test_set)\n",
    "print('Accuracy        :',(accuracy))\n",
    "print('Precision       :',(precision))\n",
    "print('Recall          :',(recall))\n",
    "print('F1-score        :',(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def process_image(img_sets):\n",
    "    # Load an image\n",
    "   # img = cv2.imread(img_sets)\n",
    "   # img_filtered = cv2.GaussianBlur(img_sets, (3, 3), 0.5)\n",
    "    #return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preditions on dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2_path = 'E:/python/deep_learning_project/dataset2'\n",
    "test_set2 = tf.keras.utils.image_dataset_from_directory(dataset2_path,  shuffle=True,\n",
    "  color_mode='rgb', image_size=(224, 224), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcac2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "test_set2 = test_set2.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32656e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_set2 = test_set2.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_set2))\n",
    "first_image = image_batch[3]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for image_batch, label_batch in test_set2:\n",
    "\n",
    "  y_true.append(label_batch)\n",
    "\n",
    "  predictions = model.predict(image_batch)\n",
    "\n",
    "  y_pred.append(np.argmax(predictions, axis = -1))\n",
    "\n",
    "correct_labels = tf.concat([item for item in y_true],axis = 0)\n",
    "predicted_labels = tf.concat([item for item in y_pred], axis =0)\n",
    "\n",
    "\n",
    "test2_cm=confusion_matrix(predicted_labels,correct_labels)\n",
    "print(test2_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82095bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_set2.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch)\n",
    "\n",
    "# Apply a sigmoid since our model returns logits\n",
    "predictions_set2 = tf.math.argmax(predictions,1)\n",
    "#predictions_ds2 = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions Dataset2:\\n', predictions_set2.numpy())\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "#plt.figure(figsize=(20, 10))\n",
    "#for i in range(32):\n",
    " # ax = plt.subplot(5, 10, i + 1)\n",
    " # plt.imshow(image_batch[i].astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert array into dataframe\n",
    "DF = pd.DataFrame(predictions_set2)\n",
    "  \n",
    "# save the dataframe as a csv file\n",
    "DF.to_csv(\"image_results_ds2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c91479",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc = model.evaluate(test_set2,verbose=2)\n",
    "print(test_loss)\n",
    "print(test_acc)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, accuracy = model.evaluate(test_set2,verbose=2)\n",
    "test_loss, precision = model.evaluate(test_set2,verbose=2)\n",
    "test_loss, recall = model.evaluate(test_set2,verbose=2)\n",
    "test_loss, f1score = model.evaluate(test_set2,verbose=2)\n",
    "print('Accuracy        :',(accuracy))\n",
    "print('Precision       :',(precision))\n",
    "print('Recall          :',(recall))\n",
    "print('F1-score        :',(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predicted_labels,correct_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2eb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
